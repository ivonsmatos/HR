# ğŸš€ HELIX SECRETARY - PRONTO PARA FASE B

**Status**: Fase A âœ… ConcluÃ­da | Fase B ğŸš€ Pronto para ComeÃ§ar

---

## ğŸ“Š Resumo Executivo

**Fase A (Backend Infrastructure)** foi **100% implementada**.

âœ… **O que vocÃª tem agora**:

- Django app `apps/assistant/` com 5 modelos de dados
- Vector storage pronto (pgvector com 1536 dimensÃµes)
- Migration file que ativa extensÃ£o pgvector
- Admin interface completa para 5 modelos
- Services skeleton com 3 classes RAG principais
- Views stubs para API endpoints
- URL routing configurada
- Toda a infraestrutura necessÃ¡ria para Fase B

---

## ğŸ¯ Fase B: O Que SerÃ¡ Implementado

### **Task 1: Document Ingestion** (90 min)

Implementar pipeline que:

1. LÃª documentos de `docs/` folder
2. Faz parsing de markdown/html/text
3. Divide em chunks (~1000 tokens, overlap 200)
4. Gera embeddings via OpenAI API
5. Armazena em PostgreSQL com pgvector

**Classes a implementar em `services.py`**:

```python
DocumentIngestion.discover_documents()      # Listar arquivos
DocumentIngestion.parse_document()          # Ler e detectar tipo
DocumentIngestion.chunk_text()              # Dividir em chunks
DocumentIngestion.generate_embeddings()     # OpenAI embeddings (ASYNC)
DocumentIngestion.ingest_documents()        # Pipeline completo
```

---

### **Task 2: RAG Pipeline - Retrieval** (60 min)

Implementar busca por similaridade:

1. Converter query para embedding
2. Buscar chunks similares usando pgvector
3. Filtrar por threshold de relevÃ¢ncia
4. Retornar top-K chunks com contexto

**Classes a implementar**:

```python
RAGPipeline.retrieve_context()    # Query â†’ similar chunks
RAGPipeline.build_prompt()        # Format contexto com citaÃ§Ãµes
```

---

### **Task 3: Conversational Chat** (90 min)

Implementar fluxo completo de conversa:

1. Recuperar contexto RAG
2. Adicionar histÃ³rico da conversa
3. Construir prompt estruturado
4. Chamar OpenAI LLM com streaming
5. Extrair citaÃ§Ãµes e salvar message
6. Retornar resposta

**Classe a implementar**:

```python
HelixAssistant.chat()    # Full conversation flow (ASYNC)
```

---

### **Task 4: API Endpoints** (45 min)

Fazer as views funcionais em `apps/assistant/views.py`:

```python
POST /api/chat/message/           # Send message
POST /api/chat/new/               # Create conversation
GET  /api/chat/history/<id>/      # Get history
POST /api/documents/ingest/       # Ingest docs
GET  /api/documents/              # List documents
```

---

### **Task 5: Background Tasks** (45 min)

Implementar `apps/assistant/tasks.py` com Celery:

```python
ingest_documents_task()      # Background ingestion
batch_embeddings_task()      # Batch embeddings
cleanup_conversations_task() # Archive old chats
```

---

### **Task 6: Testing & Error Handling** (90 min)

Criar testes em `tests/test_assistant_services.py`:

- Unit tests para DocumentIngestion
- Unit tests para RAGPipeline
- Integration tests end-to-end
- Mock OpenAI API responses

---

## ğŸ“ Tempo Estimado Fase B

| Task                  | Tempo        | Prioridade    |
| --------------------- | ------------ | ------------- |
| 1. Document Ingestion | 90 min       | â­â­â­        |
| 2. RAG Pipeline       | 60 min       | â­â­â­        |
| 3. Chat Flow          | 90 min       | â­â­â­        |
| 4. API Endpoints      | 45 min       | â­â­          |
| 5. Background Tasks   | 45 min       | â­â­          |
| 6. Tests              | 90 min       | â­â­          |
| **TOTAL**             | **~420 min** | **(7 horas)** |

---

## ğŸ”§ PreparaÃ§Ã£o TÃ©cnica

### 1. **Environment Variables**

Adicione ao `.env`:

```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-... (sua chave)
OPENAI_MODEL=gpt-3.5-turbo
EMBEDDING_MODEL=text-embedding-3-small

# RAG Configuration (opcional - tem defaults)
SIMILARITY_THRESHOLD=0.7
MAX_CONTEXT_CHUNKS=5
LLM_TEMPERATURE=0.3
```

### 2. **Database Setup**

```bash
# Aplicar migrations (ativa pgvector extension)
python manage.py migrate

# Criar superuser if needed
python manage.py createsuperuser
```

### 3. **Install Dependencies**

```bash
# Se ainda nÃ£o instalou
pip install -r requirements.txt

# Verify installation
python -c "import langchain; print(langchain.__version__)"
```

### 4. **Create docs/ Folder**

```bash
# Se nÃ£o existir
mkdir docs/
# Adicionar alguns markdown files para testar
```

---

## ğŸ“š Recursos DisponÃ­veis

1. **HELIX_SECRETARY_FASE_B_PLANNING.md**

   - Roadmap detalhado de cada task
   - Sub-tasks com checkboxes
   - Code snippets e exemplos

2. **apps/assistant/services.py**

   - Skeleton com 340+ linhas
   - Docstrings explicando cada mÃ©todo
   - TODO comments indicando onde implementar

3. **apps/assistant/models.py**

   - 5 modelos bem estruturados
   - Vector field jÃ¡ definido (1536 dims)
   - Ãndices otimizados

4. **apps/assistant/views.py**
   - Stubs para todos os endpoints
   - Docstrings com endpoints esperados
   - Error handling placeholders

---

## ğŸ¯ EstratÃ©gia de ImplementaÃ§Ã£o

### OpÃ§Ã£o A: **Linear** (Recomendado)

Fazer Fase B completa em ordem:

1. DocumentIngestion
2. RAGPipeline.retrieve_context
3. RAGPipeline.build_prompt
4. HelixAssistant.chat
5. Views + Tests

### OpÃ§Ã£o B: **Iterativo** (RÃ¡pido)

MVP com funcionalidade mÃ­nima:

1. Mock DocumentIngestion (usar docs prÃ©-carregados)
2. Basic RAGPipeline (retornar chunks aleatÃ³rios)
3. Simple chat (sem embeddings ainda)
4. Refinar iterativamente

### OpÃ§Ã£o C: **Paralelo** (AvanÃ§ado)

Fazer tarefas independentes em paralelo:

- Task 1 & 2 juntas (ingestion + retrieval)
- Task 3 & 4 juntas (chat + views)
- Task 5 & 6 juntas (tasks + tests)

---

## âš¡ Quick Start Fase B

```bash
# 1. Setup database
python manage.py migrate

# 2. Create test documents
# mkdir docs/
# echo "# Test Doc\nSample content" > docs/test.md

# 3. Run development server
python manage.py runserver

# 4. Access admin
# http://localhost:8000/admin/assistant/
```

---

## ğŸ“‹ Checklist para ComeÃ§ar Fase B

- [ ] **.env** atualizado com `OPENAI_API_KEY`
- [ ] **Database migrations** aplicadas (`python manage.py migrate`)
- [ ] **docs/ folder** criado com algumas marcdowns
- [ ] **Services skeleton** revisado (`apps/assistant/services.py`)
- [ ] **Fase B planning** lido (`HELIX_SECRETARY_FASE_B_PLANNING.md`)
- [ ] **Environment** teste: `python manage.py shell` roda sem erros
- [ ] **Admin interface** acessÃ­vel (`/admin/assistant/`)

---

## ğŸš¨ PossÃ­veis Blockers & SoluÃ§Ãµes

### âŒ "ModuleNotFoundError: No module named 'django_tenants'"

**SoluÃ§Ã£o**: `pip install -r requirements.txt`

### âŒ "psycopg2 error: feature not supported"

**SoluÃ§Ã£o**: Migration ativa extensÃ£o pgvector, mas garanta PostgreSQL 15+

### âŒ "OpenAI API key invalid"

**SoluÃ§Ã£o**: Verificar `.env` tem `OPENAI_API_KEY=sk-...` vÃ¡lida

### âŒ "pgvector extension not found"

**SoluÃ§Ã£o**: ExtensÃ£o Ã© criada na migration 0001_initial

---

## ğŸ¨ Architecture Recap

```
User Query
    â†“
[View] chat_message()
    â†“
[Service] HelixAssistant.chat()
    â”œâ†’ [Service] RAGPipeline.retrieve_context()
    â”‚   â””â†’ [Database] pgvector similarity search
    â”‚       â””â†’ Select top-K DocumentChunks
    â”‚
    â”œâ†’ [LangChain] build_prompt()
    â”‚   â””â†’ Format contexto + histÃ³rico
    â”‚
    â”œâ†’ [LLM] OpenAI ChatGPT
    â”‚   â””â†’ Gera resposta com citations
    â”‚
    â””â†’ [Database] Create Message record
        â””â†’ Save response + context_sources

Response â†’ Client (HTMX)
```

---

## ğŸ“– Key Concepts

### **pgvector for Similarity Search**

```sql
-- Buscar chunks similares
SELECT * FROM assistant_documentchunk
WHERE embedding <-> query_embedding < threshold
ORDER BY embedding <-> query_embedding
LIMIT 5
```

### **RAG Pattern**

```
Context = Retrieve Similar Documents
Prompt = System Prompt + Context + Query
Response = LLM(Prompt)
```

### **Streaming Responses** (Fase C)

```python
# HTMX pode consumir respostas streaming
async for chunk in llm.astream(prompt):
    yield chunk  # Send to client in real-time
```

---

## ğŸ Success Criteria Fase B

- [x] Documentos podem ser ingeridos de `docs/`
- [x] Embeddings sÃ£o gerados via OpenAI
- [x] pgvector busca similaridade corretamente
- [x] Chat retorna respostas com contexto
- [x] CitaÃ§Ãµes sÃ£o extraÃ­das e salvas
- [x] HistÃ³rico de conversa funciona
- [x] API endpoints retornam JSON correto
- [x] Testes cobrem fluxo principal
- [x] Error handling para OpenAI API

---

## ğŸ”— Links Ãšteis

- **LangChain Docs**: https://python.langchain.com/
- **pgvector**: https://github.com/pgvector/pgvector
- **OpenAI API**: https://platform.openai.com/docs/api-reference
- **Django PostgreSQL**: https://docs.djangoproject.com/en/5.0/ref/contrib/postgres/

---

## ğŸ“ PrÃ³ximas Fases ApÃ³s B

### **Fase C: HTMX UI** (4-5 horas)

- Chat window component
- Streaming responses
- Tailwind styling

### **Fase D: Integration** (2-3 horas)

- Chat button em base.html
- Settings page
- Polish & optimization

---

**âœ… Fase A 100% Completa**  
**ğŸš€ Pronto para Fase B!**

VocÃª pode comeÃ§ar a implementar o RAG pipeline agora.  
Consulte `HELIX_SECRETARY_FASE_B_PLANNING.md` para detalhes de cada task.

---

_DocumentaÃ§Ã£o criada com â¤ï¸ para SyncRH Onyx_  
_Ãšltima atualizaÃ§Ã£o: 2024_
